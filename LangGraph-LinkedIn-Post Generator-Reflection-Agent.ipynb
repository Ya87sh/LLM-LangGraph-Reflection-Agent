{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boiler Plate Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, List, Literal, Optional, Tuple\n",
    "from langgraph.graph import StateGraph, END\n",
    "import sys\n",
    "\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Code to use LLM with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "conversation_history = [(\"system\", \"You are a helpful assistant.\")]\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Add History\n",
    "    conversation_history.append((\"user\", user_input))\n",
    "\n",
    "    # Define Prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(conversation_history)\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    bot_response = chain.invoke({})\n",
    "\n",
    "    # Append Bot Response\n",
    "    conversation_history.append((\"assistant\",bot_response.content))\n",
    "\n",
    "    # Print using sys.stdout.write and flush\n",
    "    sys.stdout.write(bot_response.content + \"\\n\")\n",
    "    sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: Define the State (Shared Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph State\n",
    "class State(TypedDict):\n",
    "    iteration: int\n",
    "    topic: str\n",
    "    post_content: Optional[str]\n",
    "    post_content_revise: Optional[str]\n",
    "    critique_feedback: str\n",
    "    verdict: Optional[Literal[\"good\", \"rework\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: Define Prompt Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return prompt to generate LinkedIn post\n",
    "def get_generate_post_prompt() -> str:\n",
    "    \n",
    "    # Define System Content\n",
    "    systemContent = \"\"\"\n",
    "    You are an expert LinkedIn content strategist specializing in creating highly engaging posts that drive meaningful professional conversations.\n",
    "    Your task is to generate a LinkedIn post about {topic} while incorporating any previous feedback provided in {critique-feedback}.\n",
    "\n",
    "    Key Objectives:\n",
    "    Generate posts that spark professional discussions and encourage meaningful engagement\n",
    "    Maintain authenticity and avoid overly promotional language\n",
    "    Incorporate storytelling elements when relevant\n",
    "    Follow LinkedIn best practices for content structure and formatting\n",
    "    Adapt and improve based on provided critique feedback\n",
    "\n",
    "    Context Understanding:\n",
    "    Topic: {topic}\n",
    "    Previous Feedback: {critique-feedback}\n",
    "    [Note: For initial generation, feedback will be empty. For subsequent iterations, incorporate the feedback to improve the post.]\n",
    "    \"\"\"\n",
    "\n",
    "    # List of Messages for the LLM\n",
    "    messages = [(\"system\", systemContent), (\"human\", \"{topic} {critique-feedback}\")]\n",
    "\n",
    "    # Return the Prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    return prompt\n",
    "\n",
    "# Function to return prompt to critique the LinkedIn post\n",
    "def get_critique_post_prompt() -> str:\n",
    "    \n",
    "    # Define System Content\n",
    "    systemContent = \"\"\"\n",
    "    You are an expert LinkedIn content analyst specializing in evaluating professional posts for maximum engagement and impact.\n",
    "    Your task is to analyze the provided LinkedIn post and deliver two outputs: detailed critique feedback and a binary evaluation (good/rework).\n",
    "    \n",
    "    Input: {post}\n",
    "    \n",
    "    Analysis Framework:\n",
    "\n",
    "    Engagement Potential Assessment:\n",
    "    Hook strength and immediate attention grab\n",
    "    Story arc and narrative flow\n",
    "    Call-to-action effectiveness\n",
    "    Discussion potential\n",
    "    Emotional resonance\n",
    "\n",
    "    Technical Elements Review:\n",
    "    Length optimization (800-1300 characters ideal)\n",
    "    Format and readability\n",
    "    Emoji usage (appropriateness and quantity)\n",
    "    Hashtag implementation\n",
    "    Line break utilization\n",
    "\n",
    "    Content Quality Evaluation:\n",
    "    Value proposition clarity\n",
    "    Professional tone consistency\n",
    "    Authenticity markers\n",
    "    Industry relevance\n",
    "    Audience alignment\n",
    "\n",
    "    Output Structure:\n",
    "    Feedback: Detailed feedback about the post\n",
    "    Verdict: Only output either the word 'good' or 'rework' in lowercase. Remember just a single word and nothing else\n",
    "\n",
    "    Please note do not give the verdict as good when you get the post for the first time\n",
    "    \"\"\"\n",
    "\n",
    "    # List of Messages for the LLM\n",
    "    messages = [(\"system\", systemContent), (\"human\", \"{post}\")]\n",
    "\n",
    "    # Return the Prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: Define Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Node 1: Generate Post\n",
    "def generate_node_function(state: State) -> State:\n",
    "\n",
    "    # Get the Prompt\n",
    "    prompt = get_generate_post_prompt()\n",
    "\n",
    "    # Define the chain\n",
    "    generate_post_chain = prompt | llm\n",
    "\n",
    "    # Invoke the Chain        \n",
    "    response = generate_post_chain.invoke({\"topic\": state[\"topic\"], \"critique-feedback\": state[\"critique_feedback\"]})\n",
    "\n",
    "    if state[\"critique_feedback\"] == \"\":\n",
    "        # Update and Return the State\n",
    "        return {\n",
    "            **state,\n",
    "            \"post_content\": response.content\n",
    "        }\n",
    "    else:\n",
    "        # Update and Return the State\n",
    "        return {\n",
    "            **state,\n",
    "            \"post_content_revise\": response.content\n",
    "        }\n",
    "\n",
    "# Node 2: Critique Post\n",
    "def critique_post_node_function (state: State) -> State:\n",
    "\n",
    "    # Get the Prompt\n",
    "    prompt = get_critique_post_prompt()\n",
    "    \n",
    "    # Define the Chain\n",
    "    critique_post_chain = prompt | llm\n",
    "\n",
    "    # Invoke the Chain\n",
    "    if state[\"post_content_revise\"] == \"\":\n",
    "        response = critique_post_chain.invoke({\"post\": state[\"post_content\"]})\n",
    "    else:\n",
    "        response = critique_post_chain.invoke({\"post\": state[\"post_content_revise\"]})\n",
    "\n",
    "    # Get the feedback and Verdict from the response content\n",
    "    parts = response.content.split('Verdict:')\n",
    "        \n",
    "    # Extract feedback (remove 'Feedback: ' prefix and strip whitespace)\n",
    "    feedback = parts[0].replace('Feedback:', '', 1).strip()\n",
    "\n",
    "    # Extract verdict (strip whitespace)\n",
    "    verdict = parts[1].strip()\n",
    "\n",
    "    # Update Iteration\n",
    "    new_iteration = state.get(\"iteration\") + 1\n",
    "\n",
    "    # Update and Return the State\n",
    "    return {\n",
    "        **state,\n",
    "        \"critique_feedback\": feedback,\n",
    "        \"verdict\": verdict,\n",
    "        \"iteration\": new_iteration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: Continue Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_condition(state:State) -> str:\n",
    "\n",
    "    # Get for verdict\n",
    "    if state[\"verdict\"].lower() == \"rework\" and state[\"iteration\"] < 6:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"stop\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: Define Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes to the workflow\n",
    "workflow.add_node(\"generate_post_node\", generate_node_function)\n",
    "workflow.add_node(\"critique_post_node\", critique_post_node_function)\n",
    "\n",
    "# Add a directed edge\n",
    "workflow.add_edge(\"generate_post_node\",\"critique_post_node\")\n",
    "\n",
    "# Add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"critique_post_node\",\n",
    "    lambda x: continue_condition(x),\n",
    "    {\n",
    "        \"continue\": \"generate_post_node\",\n",
    "        \"stop\": END\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"generate_post_node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: Compile and Execute Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "app = workflow.compile()\n",
    "\n",
    "# Get the user input about the topic of post\n",
    "user_input = input(\"On which topic you want me to generate a LinkedIn post: \")\n",
    "\n",
    "# Initialize state\n",
    "state = {'iteration': 0, 'topic': user_input, 'critique_feedback': \"\", 'post_content_revise': \"\"}\n",
    "\n",
    "# Process through workflow\n",
    "result = app.invoke(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: Print the Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original LinkedIn post about {user_input}:\")\n",
    "\n",
    "print(result.get(\"post_content\"))\n",
    "\n",
    "print(f\"\\n Revise LinkedIn post about {user_input}:\")\n",
    "\n",
    "print(result.get(\"post_content_revise\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
